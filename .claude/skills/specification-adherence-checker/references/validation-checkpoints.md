# Specification Validation Checkpoints

Mandatory validation gates to prevent architectural drift and ensure specification compliance.

## Pre-Implementation Validation Gates

Execute these checkpoints **BEFORE writing ANY code** for every feature, bug fix, or refactoring.

---

### Gate 1: Specification Reference Checkpoint

**Purpose**: Ensure clear understanding of what needs to be implemented

**Checklist**:
```
‚ñ° Specification document identified and read
‚ñ° Specific requirement/feature section located
‚ñ° Requirement ID or section number documented
‚ñ° All acceptance criteria understood
‚ñ° Success metrics defined
‚ñ° Edge cases identified
```

**Questions to Answer**:
1. What specific requirement am I implementing?
2. Where is this documented in specifications?
3. What are the exact acceptance criteria?
4. How will I know when this is complete?

**Red Flags (Trigger uncertainty-protocol-enforcer)**:
- ‚ùå Can't find specification for this feature
- ‚ùå Requirement is ambiguous or unclear
- ‚ùå Multiple interpretations possible
- ‚ùå Success criteria not defined

**Example**:
```
‚úÖ GATE 1 PASSED: Specification Reference

Requirement: User Authentication (spec.md, Section 3.2)
Requirement ID: AUTH-001
Acceptance Criteria:
  - Users can log in with email and password
  - Invalid credentials return 401 status
  - Successful login returns JWT token
  - Token expires in 1 hour
Success Metric: User can authenticate and receive valid JWT
```

---

### Gate 2: Architecture Compliance Checkpoint

**Purpose**: Validate approach aligns with documented architecture

**Checklist**:
```
‚ñ° Architecture documentation reviewed
‚ñ° Relevant patterns identified
‚ñ° Layer placement determined
‚ñ° Integration points identified
‚ñ° No pattern violations detected
‚ñ° Naming conventions understood
```

**Questions to Answer**:
1. Which architectural patterns apply to this feature?
2. What layer(s) will this be implemented in?
3. Which existing components will this integrate with?
4. Are there established patterns for similar features?

**Red Flags (Trigger drift detection)**:
- ‚ùå Proposed approach bypasses architectural layers
- ‚ùå Different pattern than established conventions
- ‚ùå Creates coupling between layers that should be separated
- ‚ùå Violates dependency injection or other core patterns

**Example**:
```
‚úÖ GATE 2 PASSED: Architecture Compliance

Architecture Pattern: Three-tier (Presentation ‚Üí Business ‚Üí Data)
Implementation Layers:
  - Presentation: AuthController (handles HTTP)
  - Business: AuthService (handles auth logic)
  - Data: UserRepository (handles user data access)
Integration Points:
  - JwtService (for token generation)
  - PasswordHashingService (for password validation)
Pattern Compliance: ‚úì Repository pattern, ‚úì DI via constructor
```

---

### Gate 3: Design Pattern Adherence Checkpoint

**Purpose**: Ensure consistent use of established design patterns

**Checklist**:
```
‚ñ° Existing code patterns reviewed
‚ñ° Similar implementations studied
‚ñ° Naming conventions identified
‚ñ° Code structure patterns understood
‚ñ° Testing patterns identified
‚ñ° No pattern deviations detected
```

**Questions to Answer**:
1. How are similar features implemented in the codebase?
2. What naming conventions are used?
3. What code structure patterns exist?
4. How are similar features tested?

**Red Flags (Trigger drift detection)**:
- ‚ùå Different naming convention than codebase
- ‚ùå Different code structure than similar features
- ‚ùå Inconsistent error handling patterns
- ‚ùå Different testing approach than established

**Example**:
```
‚úÖ GATE 3 PASSED: Design Pattern Adherence

Naming Conventions:
  - Classes: PascalCase (e.g., AuthService)
  - Methods: camelCase (e.g., authenticateUser)
  - Constants: UPPER_SNAKE_CASE (e.g., TOKEN_EXPIRY)
Code Structure:
  - Services use constructor DI
  - Controllers use async/await
  - Repositories return domain objects, not DTOs
Testing Patterns:
  - Unit tests in __tests__/ directory
  - Mocks via jest.mock()
  - Integration tests use test database
```

---

### Gate 4: Interface Contract Validation Checkpoint

**Purpose**: Verify all interface contracts will be honored

**Checklist**:
```
‚ñ° API contracts identified
‚ñ° Request/response formats documented
‚ñ° Error handling specified
‚ñ° Status codes defined
‚ñ° Data types validated
‚ñ° No breaking changes detected
```

**Questions to Answer**:
1. What API contracts must be honored?
2. What request/response formats are specified?
3. What error formats are expected?
4. Are there existing consumers of these interfaces?

**Red Flags (Trigger drift detection)**:
- ‚ùå Response format differs from specification
- ‚ùå Additional fields added to API response
- ‚ùå Different error format than documented
- ‚ùå Breaking changes to existing contracts

**Example**:
```
‚úÖ GATE 4 PASSED: Interface Contract Validation

API Contract: POST /api/auth/login
Request Format:
  {
    "email": "string (required, email format)",
    "password": "string (required, min 8 chars)"
  }
Response Format (200 OK):
  {
    "token": "string (JWT)",
    "expires_in": "number (seconds)"
  }
Error Format (401 Unauthorized):
  {
    "error": "string (error message)"
  }
Breaking Changes: None (new endpoint)
```

---

### Gate 5: Quality Criteria Verification Checkpoint

**Purpose**: Confirm understanding of quality standards

**Checklist**:
```
‚ñ° Test coverage requirements known
‚ñ° Performance requirements identified
‚ñ° Security requirements understood
‚ñ° Accessibility requirements defined
‚ñ° Documentation requirements clear
‚ñ° Compliance requirements identified
```

**Questions to Answer**:
1. What test coverage is required?
2. Are there performance requirements?
3. What security measures are needed?
4. Are there compliance requirements (GDPR, HIPAA, etc.)?

**Red Flags (Trigger uncertainty-protocol-enforcer)**:
- ‚ùå Test coverage requirements unclear
- ‚ùå Performance SLAs not defined
- ‚ùå Security requirements ambiguous
- ‚ùå Compliance needs unknown

**Example**:
```
‚úÖ GATE 5 PASSED: Quality Criteria Verification

Test Coverage: Minimum 80% line coverage required
Performance: Login must complete in <500ms (p95)
Security:
  - Passwords hashed with bcrypt (cost factor 12)
  - JWT signed with RS256 algorithm
  - HTTPS required in production
Accessibility: N/A (API endpoint)
Documentation: OpenAPI spec must be updated
Compliance: GDPR compliant (no PII logging)
```

---

## During Implementation Validation

Execute these checkpoints **WHILE writing code** to catch drift early.

---

### Continuous Specification Reference

**Every 15-30 minutes during implementation**:

```
‚ñ° Re-read specification section
‚ñ° Verify current code matches spec
‚ñ° Check no features added beyond spec
‚ñ° Validate data formats match spec
‚ñ° Confirm validation rules match spec
```

**Purpose**: Prevent gradual drift during implementation

---

### Incremental Pattern Validation

**After completing each component** (class, function, module):

```
‚ñ° Compare to similar existing code
‚ñ° Verify naming consistency
‚ñ° Check pattern adherence
‚ñ° Validate DI usage
‚ñ° Confirm error handling consistency
```

**Purpose**: Maintain pattern consistency throughout implementation

---

### Interface Contract Monitoring

**Before committing any API changes**:

```
‚ñ° Verify request format unchanged (or properly versioned)
‚ñ° Verify response format matches spec
‚ñ° Check error formats consistent
‚ñ° Validate status codes correct
‚ñ° Confirm no breaking changes introduced
```

**Purpose**: Prevent accidental contract violations

---

## Post-Implementation Validation

Execute these checkpoints **AFTER writing code** before marking feature complete.

---

### Gate 6: Specification Compliance Review

**Purpose**: Final validation that implementation matches specification exactly

**Checklist**:
```
‚ñ° All specified features implemented
‚ñ° No features added beyond specification
‚ñ° No features simplified or removed
‚ñ° All acceptance criteria met
‚ñ° All edge cases handled as specified
‚ñ° Data formats match specification
‚ñ° Validation rules match specification
```

**Review Process**:
1. Read specification section line-by-line
2. For each requirement, verify implementation
3. List any deviations (even minor ones)
4. If deviations exist, categorize as:
   - Drift (requires fix or authorization)
   - Clarification (specification was ambiguous)
   - Enhancement (beyond scope, requires approval)

**Example**:
```
‚úÖ GATE 6 PASSED: Specification Compliance Review

Specification: User Authentication (AUTH-001)

Requirements Review:
‚úì Users can log in with email and password (AuthController.login)
‚úì Invalid credentials return 401 (line 45-47)
‚úì Successful login returns JWT token (line 52)
‚úì Token expires in 1 hour (JwtService.generateToken, expiry: 3600s)

Edge Cases:
‚úì Empty email/password ‚Üí 400 Bad Request
‚úì Malformed email ‚Üí 400 Bad Request
‚úì Account locked ‚Üí 403 Forbidden
‚úì Server error ‚Üí 500 Internal Server Error

Deviations: NONE

Specification Compliance: 100%
```

---

### Gate 7: Architecture Integrity Validation

**Purpose**: Confirm no architectural drift occurred during implementation

**Checklist**:
```
‚ñ° All layers respected (no layer violations)
‚ñ° Patterns followed consistently
‚ñ° Dependency injection used correctly
‚ñ° No tight coupling introduced
‚ñ° Integration points clean
‚ñ° Naming conventions consistent
```

**Validation Method**:
1. Draw actual implementation architecture diagram
2. Compare to documented architecture
3. Identify any deviations
4. Assess impact of deviations

**Example**:
```
‚úÖ GATE 7 PASSED: Architecture Integrity Validation

Implemented Architecture:
  AuthController (Presentation)
    ‚Üì depends on
  AuthService (Business)
    ‚Üì depends on
  UserRepository (Data) + JwtService (Utility)

Comparison to Documented Architecture:
‚úì Three-tier pattern maintained
‚úì No layer-skipping dependencies
‚úì Constructor DI used throughout
‚úì Services depend on interfaces, not implementations
‚úì No circular dependencies

Architecture Drift: NONE
```

---

### Gate 8: Integration Testing Validation

**Purpose**: Verify integration with existing system works correctly

**Checklist**:
```
‚ñ° All integration points tested
‚ñ° Interface contracts validated with tests
‚ñ° Existing functionality not broken
‚ñ° End-to-end flows work
‚ñ° Error propagation correct
‚ñ° No side effects detected
```

**Testing Strategy**:
1. Test each integration point individually
2. Test end-to-end user flows
3. Test error scenarios
4. Run full regression test suite
5. Validate performance under load

**Example**:
```
‚úÖ GATE 8 PASSED: Integration Testing Validation

Integration Points Tested:
‚úì AuthController ‚Üí AuthService (unit + integration tests)
‚úì AuthService ‚Üí UserRepository (integration tests)
‚úì AuthService ‚Üí JwtService (unit tests with mocks)
‚úì Full login flow (e2e test)

Regression Testing:
‚úì All existing tests pass (1,247 tests, 0 failures)
‚úì No new warnings or errors
‚úì Performance benchmarks within acceptable range

Breaking Changes: NONE
```

---

## Drift Response Protocol

When drift detected at any checkpoint:

### Step 1: HALT Implementation

```
üö´ DRIFT DETECTED AT CHECKPOINT

Gate: [Gate name]
Issue: [Specific drift detected]
Severity: [Critical / Major / Minor]

IMPLEMENTATION HALTED - Awaiting resolution
```

### Step 2: Analyze Drift

```
Drift Analysis:

Specification: [Quote exact specification]
Implementation: [What was actually done]
Deviation: [Specific difference]
Impact: [What this affects]
```

### Step 3: Determine Resolution Path

**Option A: Fix Drift (Preferred)**
- Modify implementation to match specification exactly
- Re-run checkpoints to validate compliance
- Continue implementation

**Option B: Update Specification (Requires Authorization)**
- Document why specification should change
- Request client approval for specification update
- Update specification documents
- Continue with updated spec

**Option C: Document as Technical Debt (Requires Authorization)**
- Trigger `technical-debt-evaluator` Skill
- Get client authorization for debt
- Log debt in memory system
- Plan debt paydown timeline

### Step 4: Log Drift Event

```bash
python scripts/log_drift_prevention.py \
  --type "[drift-type]" \
  --specification "[spec quote]" \
  --proposed "[what was attempted]" \
  --resolution "[how resolved]"
```

---

## Checkpoint Timing Guide

### For Small Features (< 4 hours)

- **Pre-Implementation**: All 5 gates (30-45 minutes)
- **During Implementation**: Continuous reference every 30 min
- **Post-Implementation**: Gates 6-8 (30-45 minutes)

**Total Overhead**: ~1.5-2 hours (but prevents hours of rework)

---

### For Medium Features (4-8 hours)

- **Pre-Implementation**: All 5 gates (45-60 minutes)
- **During Implementation**: Continuous reference + incremental validation every hour
- **Post-Implementation**: Gates 6-8 (45-60 minutes)

**Total Overhead**: ~2-3 hours (but prevents days of rework)

---

### For Large Features (> 8 hours)

- **Pre-Implementation**: All 5 gates + detailed planning (1-2 hours)
- **During Implementation**: Checkpoints at each milestone
- **Post-Implementation**: Gates 6-8 + comprehensive review (1-2 hours)

**Total Overhead**: ~3-5 hours (but prevents weeks of rework and technical debt)

---

## Checkpoint Efficiency Tips

### Batch Related Checkpoints

Instead of running gates individually, batch related checks:

- **Gates 1-2**: Specification + Architecture (together)
- **Gates 3-4**: Patterns + Contracts (together)
- **Gates 6-7**: Compliance + Integrity (together)

### Use Checklists

Maintain checkpoint checklists in:
- `/docs/specification-validation-checklist.md`
- Project-specific validation docs
- Memory system for reuse across sessions

### Learn from History

Check `/memories/protocol-compliance/drift-prevention-log.xml` for:
- Common drift patterns in this project
- Previously validated patterns
- Approved exceptions
- Lessons learned

---

## Integration with Other Protocols

### With `test-driven-development` Skill

1. Write tests FIRST (TDD)
2. Tests validate specification compliance
3. Run checkpoints before implementation
4. Implementation guided by tests + checkpoints
5. Post-implementation validation confirms both tests and specs

### With `systematic-debugging` Skill

1. When bugs found, check for specification drift
2. Was bug caused by drift from spec?
3. Fix bug by returning to specification
4. Log bug + drift pattern for future prevention

### With `surgical-precision-guide` Skill

1. Use surgical levels within specification constraints
2. Minimalist approach = exact spec, no more
3. Level 1-3: Simple compliance
4. Level 4+: Complex compliance (needs approval)

---

**Last Updated**: 2025-10-29
**Skill Version**: 1.0.0
